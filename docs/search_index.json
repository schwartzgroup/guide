[["index.html", "Joel’s lab guide 1 About", " Joel’s lab guide 2024-12-18 1 About This Bookdown serves to collect information that may be useful for your regular interactions with the lab machines. To navigate, use the buttons at the left. "],["getting-started.html", "2 Getting started 2.1 Connecting to the VPN 2.2 Topology of the lab servers 2.3 Best practices 2.4 Connecting to RStudio 2.5 Connecting to the command line of a server", " 2 Getting started This section will help you get started using the lab servers. 2.1 Connecting to the VPN If you’re not currently in the 401 Park / Landmark building, you’ll have to first connect to the Harvard VPN using your HarvardKey credentials to access the lab servers. If you don’t have the VPN client already, you can download it at https://vpn.harvard.edu. AnyConnect functionality comes pre-installed in Linux systems (OpenConnect -&gt; Cisco AnyConnect in the NetworkManager VPN manager). The VPN address is vpn.harvard.edu/hsph and your username is your Harvard email. You will be prompted to enter two passwords: first password will be your HarvardKey password and the second password will be push, which will activate a Duo push on your phone. Some notes on the VPN: If you only have a FASRC VPN account, you’ll need to get a separate, generic Harvard VPN account. FASRC VPN accounts seem to be unable to access the lab servers. If you’re still having trouble, try to email IT and ask to be added to the HSPH tunnel as your account might not yet have access. 2.2 Topology of the lab servers The lab’s servers consist primarily of two categories of servers: Work servers, which have high CPU and memory capacity, and Storage servers, which have high disk capacity but low CPU and memory capacity. Each of the storage servers is accessible from the /media/ directory of each work server. For example, the qnap5 storage server is available by accessing /media/qnap5/ on any of the work servers, Isilon is available at /media/Isilon, etc. The structure of /media/ is set up such that it is identical on each of the work servers, so code that references e.g. /media/qnap4/some-file.txt will be able to run without modification on any of the work servers. For detailed information about what servers are available and what resources each possesses, see the section on server attributes. 2.3 Best practices In general, you will want to do your work (i.e. running R code) on one of the work servers and save data to / load data from one or more of the storage servers. The connections of the servers are fast enough that speed shouldn’t be an issue. The work servers have limited local space, and this space should generally be reserved for R libraries, RStudio temporary files, etc. which are necessary for the RStudio Server to function. It’s OK to keep a few small files in your /home/ directory (which is unique to each of the work servers), but don’t go too crazy! For guidance on which server to pick for your work, see the section on how to pick a server to work on. 2.4 Connecting to RStudio RStudio is available on port 8787 of any of the work. You can access RStudio directly by clicking on one of the links below (helpful to bookmark!): tiamat: http://sph-tiamat.sph.harvard.edu:8787/ or http://10.174.192.10:8787/ mithra: http://sph-mithra.sph.harvard.edu:8787/ or http://10.174.192.11:8787/ dagda: http://sph-dagda.sph.harvard.edu:8787/ or http://10.174.192.12:8787/ shiva: http://sph-shiva.sph.harvard.edu:8787/ or http://10.174.192.14:8787/ GCMC: http://sph-gcmc-js.sph.harvard.edu:8787/ or http://10.174.192.15:8787/ Accounts on GCMC must be requested from William Kessler. 2.5 Connecting to the command line of a server In some cases, you may need to connect directly to the server command lines rather than the RStudio Servers, e.g. to kill jobs, install software, move around a lot of files, etc. RStudio Server has a Terminal tab next to the Console tab that you can use, but it may be more convenient to connect directly via SSH (“Secure Shell”, the protocol for connecting to Linux and Unix servers). To connect via SSH on macOS or Linux, open the terminal application and type ssh your_username@server_ip, e.g. ssh jharvard@10.174.192.10. To connect via SSH on Windows, you can either: Install PuTTY, MobaXTerm, or some other terminal client and then enter the server IP address into the main dialog, or Install Linux and then type ssh your_username@server_ip from the Linux terminal "],["server-information.html", "3 Server information 3.1 Server attributes 3.2 Server resource monitor 3.3 How to pick a server to work on 3.4 More precise monitoring", " 3 Server information This section lists information about the various servers and how to assess their status + suitability for work. 3.1 Server attributes The following table lists attributes of the servers as of March 23, 2023. type name ip CPU threads max clock memory capacity Work tiamat 10.174.192.10 24 48 3.2 GHz 512 GB 1.8 TB Work mithra 10.174.192.11 20 40 4.5 GHz 384 GB 1.9 TB Work dagda 10.174.192.12 16 16 2.8 GHz 128 GB 220 GB Work shiva 10.174.192.14 24 48 3.2 GHz 512 GB 1.8 TB Work gcmc-js 10.174.192.14 11 22 3.2 GHz 756 GB 116 GB Storage Isilon 10.174.191.128 unk. unk. unk. unk. 245 TB Storage qnap2 10.174.192.22 unk. unk. unk. unk. 29 TB Storage qnap3 10.174.192.23 4 4 1.7 GHz 2 GB 44 TB Storage qnap5 10.174.192.23 unk. unk. unk. unk. 26 TB Notes: capacity for computational machines refers to that of the home directory; for storage machines, it refers to the total size of the hard drive pool. To assess CPUs: sudo dmidecode -t processor To assess memory: sudo dmidecode -t memory To assess disk: df -h Machine-specific notes: gate, qnap, and qnap4 no longer exist and the data has been migrated to the Isilon machines. The original mount points of /media/gate/, /media/qnap/, and /media/qnap4/ still exist but are just symbolic links to directories in /media/Isilon/. Isilon is shared storage for the whole school. The full storage capacity may not be available. gcmc-js is a virtual machine, so performance may be degraded compared to the bare metal machines. 3.2 Server resource monitor Munin is available on the servers for resource monitoring purposes. This can be used, for example, to: Determine what machines are available for running jobs; Help debug the causes of crashes and freezes; and Know ahead-of-time what potential problems may cause a machine to crash before it does. Munin is hosted on the dagda machine due to its low utilization compared to the other machines. To access Munin, navigate to http://sph-dagda.sph.harvard.edu:8080/munin http://10.174.192.12:8080/munin/ in your browser. You will be greeted with the following screen: On the left-hand side of the main page is the navigation pane. From here, you can navigate to dashboards that summarize various properties of the machines. At the top left are some quick links to the most useful dashboards. Underneath that, another section of interest is the problems section, which will list any notices regarding the health of the machines. On the right-hand side is the page content. Normally, various graphs will appear here. On the main page, there are instead some links to machine-specific dashboards, with any problems highlighted in red or yellow. The following sections will dive into specific Munin dashboards. 3.2.1 Load average The load average refers to the current average utilization of processing power. A load of 0% means that the machine is idle, while a load of 100% means that the machine is fully being used. The load average can go above 100% - this means that processes are requesting more power than the machine can provide and the kernel is trying to distribute work fairly among processes. This means that, as a simplified example, if there are two programs competing for all of the available threads, then the kernel may decide to pause one program, work on the other, pause that, return to the first program, and repeat until more resources become available. An example of a load average graph is shown below - notice how it briefly peaks above 100%: To see the current load averages across all of the servers, see http://10.174.192.12:8080/munin/system-day.html#Load%20average or click the Load (d) link on Munin. 3.2.2 Memory utilization Memory utilization is a very complicated topic, in part because the kernel does so much work behind the scenes so that programs don’t have to worry about the finer details of memory management. This subsection will focus specifically on two main aspects of memory use: unused: Memory marked as unused exists in RAM and is freely available for programs to use. In an ideal situation, there will be enough unused memory for your programs, and under normal operation there will usually be some unused memory. swap: Memory marked as swap exists in your swap space, which is a portion of the disk that is set aside to accommodate extra memory. Swap memory is extremely slow in comparison to unused memory - typical RAM can max out at 12 GB/s for DDR3 memory and over 32 GB/s for DDR4 memory, while a typical hard drive maxes out at around 160 MB/s. For computers with SSDs, the difference is not as egregious but still substantial. Swap memory should only be used as a last resort, and, indeed, the kernel considers it as such. When there is no unused memory, the kernel will start transferring memory from your RAM to your swap space in a process known as swapping. The effect of this is that programs using that memory will be extremely slowed down, because the swapped memory must first be transferred back to the RAM before it can be used. If a machine starts swapping, keep an eye on it! A little swap can be OK, but once all of the swap space is used, the machine has run out of memory and the kernel will start terminating programs. The process that does this is called the out-of-memory (OOM) killer, which calculates the badness for each process by weighing the amount of memory it uses vs. how long it has been running. The exact algorithm for determining badness can be found on the linked page. In most cases, your program will have the most badness and will be killed by the OOM killer vs. other system processes that have been running for a long time (and therefore have less badness), so it’s important to pay attention! An example of a memory utilization graph is shown below - notice how there are some brief periods where the computer starts swapping (at one point it even gets so bad that the monitoring software can’t keep up and a gap shows up in the graph!): To see the current memory utilization across all of the servers, see http://10.174.192.12:8080/munin/system-day.html#Memory%20usage or click the Memory (d) link on Munin. 3.3 How to pick a server to work on Is it capable? Refer to the server attributes table and ensure that the server has enough processing speed and memory. Not sure how much memory your code might use? Do a short test run and monitor the resource usage closely (see the next section) - if it ends up using more memory than you expect then you can kill it before it freezes up the server and gets killed on its own. For running simple models: max clock is the most important attribute. This is because functions like glm(), survival::clogit(), nlme::lme(), etc. only run on a single core. For running parallel jobs: multiply max clock by the number of threads. Each thread will be executing code simultaneously. Caution: in general most parallel jobs will create a full copy of your data set before running so that each subprocess has something to work with. Be sure to carefully monitor your memory usage if running parallel jobs with a large data set, or running a large number of parallel jobs on a small data set! Is there room? Refer to the server resource monitor section and ensure that the actual amount of processing power and memory available is enough for your job. Monitor your job. Be sure to revisit #2 occasionally as your jobs are running! Other people may have started jobs on the same server and it may be faster to switch to a different one. The amount of memory available and what your job requires may be different from what you originally thought. If the machine starts swapping, be on the lookout! Swap memory is extremely slow and should be used sparingly. Additionally, once the swap is fully used, the kernel will start terminating programs. 3.4 More precise monitoring While the server resource monitor is useful for getting a quick overview of the state of all of the work servers, there are some key limitations: The resource monitor only updates every 5 minutes. This means that jobs that cause a very quick resource spike may not be able to be detected before they start causing issues. Per-thread CPU monitoring isn’t available. It may be more instructive to see per-thread utilization rather than a machine load average when, for example, deciding whether or not to parallelize a job or seeing if a single-core function may be better replaced by a quicker one (e.g. an nlme::lme() on large data vs. a mgcv::bam()). In the above cases, htop is your best friend. htop can be accessed by logging in to the command line and typing htop. You will be presented with a window like the one below: In htop, the meters at the top left represent the utilization of each individual core and are followed by meters for RAM and swap. Below that, you can see a table of all processes along with the user running them and some information about CPU and memory utilization. Clicking on cells in the header row will sort the processes by that column. In the above example, we can see that the table is being sorted by CPU utilization. You can also press F4 or click the Filter button at the bottom to enter a search term to filter rows by. "],["troubleshooting.html", "4 Troubleshooting 4.1 Troubleshooting file permissions errors 4.2 Troubleshooting the RStudio Server login", " 4 Troubleshooting This section lists information about how to troubleshoot various issues that might arise during regular usage. 4.1 Troubleshooting file permissions errors User IDs are not synced across the servers, so creating a directory / file on one server may prevent your account on other servers from modifying that file / directory. To fix this, log in to the terminal and run chmod -Rv a+rw /path/to/target on your file / directory to allow all other users to make modifications. Alternatively, if you have superuser, you can use sudo chmod from any account, or you can ask someone with superuser to do that for you. 4.2 Troubleshooting the RStudio Server login Several issues can arise when using the RStudio Server related to saturating the CPU, memory, or I/O. If you’re having issues logging in, but the RStudio login page shows up, this means that the machine and RStudio Server are both active and the issue is likely tied to your specific live sesion. To resolve this, you can try to post to the lab Google Group or log in to the command line and try one of the following (listed in order of likeliness): Kill running jobs: Use htop and press F4 to filter by rsession -u [your username] to see if you have a job open. If you see any jobs using up 100% CPU then likely they’re holding up the login process and the server is waiting for whatever you were running the last time you logged on to finish. If this is the case, you can either wait or press F9 to open the kill job menu and send signal 9 (SIGKILL) to terminate immediately. Free up memory: Use htop to see if the Mem bar is full. If it is, then your session is likely swapping which means it’s processing data on-disk instead of in-memory. This is extremely slow. If all servers look like this and the CPUs look idle despite the memory usage, try to reach out to the people logged in to the RStudio Server and see if they’re still running anything. Sometimes people forget about their jobs (no problem with that, of course!) and superusers can kill them to free up some memory. Delete session cache: Use du -sh ~/.local/share/rstudio/sessions/ to see the size of your saved sessions. Every time you log in, RStudio Server restores your last session using these files. If it’s bigger than say 10GB and you don’t want to wait you can use rm -rf ~/.local/share/rstudio/sessions/ to remove them. Check for unmounted drives: Use mount -a and make sure the drive holding the data that you were using is there. If it’s not, then one of the network drives disconnected. As a result, your process likely got zombified and the server needs to be restarted. "],["transferring-files.html", "5 Transferring files 5.1 Simple transfers: scp 5.2 Large transfers: rsync 5.3 Interacting with the cloud: rclone", " 5 Transferring files Last updated March 24, 2022 This guide contains some instructions for transferring files securely via SSH to Linux and other Unix/Unix-like servers. Formatting notes: Throughout the document I will be using monospace font to refer to commands, command arguments, files, and other keywords. Every section will have an example located right after the paragraph that begins with the bolded words For example, so if you’re in a rush you can just CTRL-F! 5.1 Simple transfers: scp scp (“secure copy”) is a program that exists by default on all modern UNIX-like systems and uses an SSH connection to transfer data to a remote server. This is what most SSH consoles use in the background, I believe - MobaXTerm, PuTTY, etc. Basic usage is similar to the normal copy command and takes the form: scp [-options] source dest where source and dest are the source and destination paths (relative to the home folder) and -options is an optional list of options. The “remote” part comes into play by specifying a prefix for the source, destination, or both that indicates what machine the path is pointing to, if not the current machine, by using user@hostname.or.ip.address:. For this tutorial I have aliased the server IP addresses to joel-tiamat, joel-mithra, etc. using my SSH config, but that is beyond the scope of this tutorial. 5.1.1 Transferring files For example, if I want to copy the file a.txt, in the current directory, from my computer to my home directory on one of the servers: scp test/a.txt joel-tiamat:/home/edgar/ What this looks like in practice: Note that you are able to see the progress of the transfer while it is transferring. In the above screenshot it just shows up as “100%” but in practice this will start at 0% and update every few milliseconds. Similarly, I can copy the file back by simply reversing the order of paths: scp joel-tiamat:/home/edgar/a.txt ~/test/ Or, I can transfer the copy from one machine to a different remote machine: scp joel-tiamat:/home/edgar/a.txt joel-mithra:/home/edgar/ The last command might be useful if you are transferring a big file that you do not have space for on your computer if you cannot grant direct access from one machine to the other – otherwise, it’s always faster to just do a direct transfer between the machines because it takes your computer out of the equation. 5.1.2 Transferring folders If you want to transfer a folder instead of a file, you can use the same command but you must now add the -r (recursive) scp -r source dest For example, say I want to copy the entire folder containing a.txt instead of just a.txt: scp -r test/ joel-tiamat:/home/edgar Output is the same as before, but you can now see multiple files being transferred: As before, you can copy something to your machine instead of from your machine by reversing the paths. 5.1.3 Other scp options scp has a few other options that you can apply to tweak the behaviour of the command, which you can access by typing man scp (“manual for scp”). I have highlighted key options below: -C (upper case C): Enable compression of the file in transit. This requires your machine and the destination machine to both be powerful enough to compress the data faster than it is being sent and this may not always be the case – I personally don’t use this. -i identity_file: Use the specified identity file to connect to the remote machine. This is necessary if the remote machine does not allow password-based logins. -l limit: Limit the transfer speed to this many Kbit/s -q: Disable the live progress indicator. This is useful for saving space if you are using a shared cluster with something like SLURM, because otherwise all of the progress would be dumped to a text file that could grow large quickly. 5.2 Large transfers: rsync If you are transferring relatively large folders, rsync (“remote sync”) is preferable to scp because rsync only transfers items that have not changed since the last transfer. This is useful if, for example, you are transferring a very large data set where parts of the data set are updated periodically, but not the whole thing. rsync does this by: Checking if the destination folder exists, If it does, checking if the file is absent, and If present, checking if the remote file has a different modification time or size. If any of the above checks are violated, the file is copied over. Otherwise, the file is not transferred. The rsync command is very similar to scp and uses essentially the same format: scp -ave ssh [-options] source dest where the -ave ssh here means: -a: archive the files (make a copy) -v: be verbose (show me what’s happening) -e ssh: establish a connection using SSH Though similar, rsync differs from scp in that you must provide the path to where you want the folder to exist, followed by a forward slash (/), rather than the folder that it exists in. This may be a little confusing but will be cleared through the example. For example, let’s say I want to transfer the same folder as before to the same destination. I have removed it from the destination for demonstration purposes. The command I would use is: rsync -ave ssh test/ joel-tiamat:/home/edgar/test/ The output is as follows: There is something very important to point out here: instead of before, where I specified the folder where I want the folder to be transferred into, i.e. /home/edgar/, I am specifying the exact path that the folder will appear at, i.e. /home/edgar/test/. If you had instead specified the same destination as in scp, you would find that rsync would have dumped the contents of the source folder into the destination folder, rather than creating a new folder inside the destination folder and dumping contents into that. Now, let’s say I made a change to a file or added a new file and want to sync the changes. Notice what happens when I run the same command: Only the changed file (a.txt) and a new file (c.txt), were copied! b.txt did not change between transfers and was not re-copied. This can be a huge time-saver and also help to save you money if you are being charged based on your bandwidth. 5.2.1 Other rsync options As before, here is a list of useful rsync options: * --compress: Compress files being transferred. Similar to scp -C. * --copy-links: Also copy links / shortcuts. * --progress: Show the progress of the file currently being transferred. Unlike scp, there is no progress indicator by default. I usually add this to every rsync command because it’s easy to see if a transfer has locked up / frozen. * --remove-source-files: Remove files from the source folder after they are transferred. This essentially turns rsync into a move command instead of a copy command. 5.3 Interacting with the cloud: rclone rclone is yet another tool for remote transfers that can handle interaction with cloud services, e.g. AWS S3, Google Drive, Microsoft Azure Blob Storage, etc. Unlike rsync and scp, rclone is not installed by default and you must grab a copy from rclone.org. I will not go into this extensively here as the rclone website has extensive documentation available, but let me know if you need any help! "],["special-instructions-for-specific-software.html", "6 Special instructions for specific software 6.1 Gurobi 6.2 fbkmr 6.3 RQPD", " 6 Special instructions for specific software Last updated March 27, 2023 Some software require special steps for their installation or use. This chapter aims to document them. 6.1 Gurobi The Gurobi package has some peculiarities involved in its installation including manually installing binaries and libraries to /opt/ and having to edit the PATH and LD_LIBRARY_PATH search paths as a result. 6.1.1 Use On the Gurobi website, first create an account and make use of your product key, which is of the format XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX. Once everything is installed, each user will need to register their product key using the following code: grbgetkey XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Note that the per-user registration only works on one machine at a time. If the above code works, then nothing else needs to be done. The following section details how to install Gurbo and enable the above command on machines that do not yet have it. 6.1.2 Installation 6.1.2.1 Download Gurobi and upload to the server From the Gurobi Optimizer download page, download the latest Gurobi Optimizer version for x64_Linux. For this guide, we will be installing version 9.5.1, so we will be downloading gurobi9.5.1_linux64.tar.gz You can either download from your browser and then upload to one of the servers, or download the link directly to the server: wget https://packages.gurobi.com/9.5/gurobi9.5.1_linux64.tar.gz 6.1.2.2 Decompress to /opt/ The /opt/ directory is where manually-installed static packages should be stored, i.e. those not created with makefiles. See man hier for more information. tar xvf gurobi9.5.1_linux64.tar.gz sudo mv gurobi951 /opt/ 6.1.2.3 Enable for command-line users Command-line shells like Bash will read default environment variables from /etc/environment before loading the systemwide and user .profile and .bash_profile scripts. We want to add the newly-installed Gurobi binaries and libraries to PATH and LD_LIBRARY_PATH, which are :-delimited lists of the program and shared library search paths, respectively. Specifically, we want to add the following: To PATH: /opt/gurobi951/linux64/bin (replace gurobi951 with whatever version of Gurobi you are installing) To LD_LIBRARY_PATH: /opt/gurobi951/linux64/lib PATH may already have something there, in which case only append :/opt/gurobi951/linux64/bin, as in the following: PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/gurobi951/linux64/bin&quot; LD_LIBRARY_PATH is likely not there, so we can create a new line: LD_LIBRARY_PATH=&quot;/usr/local/lib:/opt/gurobi951/linux64/lib&quot; We also need to define GUROBI_HOME as follows: GUROBI_HOME=/opt/gurobi951/linux64 The final /etc/environment file should look something like this: PATH=&quot;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/opt/gurobi951/linux64/bin&quot; LD_LIBRARY_PATH=&quot;/usr/local/lib:/opt/gurobi951/linux64/lib&quot; GUROBI_HOME=/opt/gurobi951/linux64 6.1.2.4 Enable for RStudio Server users For whatever reason, RStudio Server does not respect /etc/environment. Therefore, we need to modify LD_LIBRARY_PATH manually. To do this, we just need to add the following line to /etc/rstudio/rserver.conf: rsession-ld-library-path=/opt/gurobi951/linux64/lib/ After that, restart RStudio Server: sudo systemctl restart rstudio-server Verify that everything is OK: systemctl status rstudio-server 6.1.2.5 Install the R package Once the above is complete, we can install the R package itself. First, we need to see where R searches for packages using .libPaths: &gt; .libPaths() [1] &quot;/usr/local/lib/R/site-library&quot; &quot;/usr/lib/R/site-library&quot; [3] &quot;/usr/lib/R/library&quot; From the above, it seems like /usr/lib/R/site-library might be a good choice. Now, we can install Gurobi and its dependency slam: # Be sure to run as root install.packages(&quot;slam&quot;, repos = &quot;https://cran.r-project.org&quot;, lib = &quot;/usr/lib/R/site-library&quot;) install.packages(&quot;/opt/gurobi951/linux64/R/gurobi_9.5-1_R_4.1.1.tar.gz&quot;, repos = NULL, lib = &quot;/usr/lib/R/site-library&quot;) Confirm that it works: &gt; library(gurobi) Loading required package: slam Warning message: package ‘gurobi’ was built under R version 4.1.1 6.2 fbkmr fbkmr does not exist on CRAN yet and must be installed manually. Ensure that first gurobi is installed (see the section on Gurobi). To decompress the file, run tar -xvf /path/to/fbkmr_0.1.0.tar.gz (or whatever the tarball is named). 6.2.1 Installing dependencies The fbkmr package comes with a file called NAMESPACE. By running the following command we can extract a list of all imported packages and print it as an R vector: LIBRARIES=$( grep import fbkmr/NAMESPACE | tr -c &quot;[:alnum:]\\n&quot; &quot; &quot; | awk &#39;{print $2}&#39; | sort | uniq | tr &quot;\\n&quot; &quot; &quot; | head -c -2 ) echo &quot;TO_INSTALL &lt;- c(\\&quot;$(sed &#39;s/ /&quot;, &quot;/g&#39; &lt;&lt;&lt; &quot;$LIBRARIES&quot;)\\&quot;)&quot; After pasting this into R, we can then install these libraries using the following code: invisible(lapply( TO_INSTALL, function(package) if (!require(package, character.only = TRUE)) install.packages( package, lib = &quot;/usr/local/lib/R/site-library&quot;, # may differ based on machine; use `.libPaths()` # lib = &quot;/opt/R/4.1.1/lib/R/library&quot;, # for example repos = &quot;https://cran.r-project.org&quot; ) )) 6.2.2 Installing fbkmr Now, we can install fbkmr using the usual code: invisible(install.packages( &quot;./fbkmr/&quot;, lib = &quot;/usr/local/lib/R/site-library&quot;, # or what is specified by .libPaths() # lib = &quot;/opt/R/4.1.1/lib/R/library&quot;, # for example repos = NULL, type = &quot;source&quot; )) 6.3 RQPD RQPD cannot be installed from CRAN and needs to be installed manually. 6.3.1 Clone the RQPD Subversion repository The RQPD source code is available on R-Forge in a Subversion repository that we can clone: svn checkout svn://scm.r-forge.r-project.org/svnroot/rqpd/ 6.3.2 Install From here, we can install RQPD. We also need to install all dependencies manually since we are not using CRAN: # Be sure to run the following as root # Install missing dependencies, if any lapply( c(&quot;stats&quot;, &quot;quantreg&quot;, &quot;Formula&quot;, &quot;Matrix&quot;, &quot;MatrixModels&quot;, &quot;SparseM&quot;), function(package) { if (!require(package)) { install.packages( package, lib = &quot;/usr/lib/R/library&quot;, repos = &quot;https://cran.r-project.org&quot; ) } } ) # Install RQPD from cloned source install.packages( &quot;./rqpd/pkg/&quot;, lib = &quot;/usr/lib/R/library&quot;, # or what is specified by .libPaths() repos = NULL, type = &quot;source&quot; ) Verify that it works: &gt; library(rqpd) Loading required package: quantreg Loading required package: SparseM Attaching package: ‘SparseM’ The following object is masked from ‘package:base’: backsolve Loading required package: Matrix Loading required package: MatrixModels Loading required package: Formula "],["datasets-exposures.html", "7 Datasets: Exposures", " 7 Datasets: Exposures "],["datasets-outcomes.html", "8 Datasets: Outcomes", " 8 Datasets: Outcomes "],["datasets-covariates.html", "9 Datasets: Covariates", " 9 Datasets: Covariates "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
